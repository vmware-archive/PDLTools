
/* ----------------------------------------------------------------------- *//**

@file prediction_metrics.sql_in

@brief Implementation of various prediction accuracy metrics.

@author Written by Michael Brand
@date 21 Oct 2014

 *//* ----------------------------------------------------------------------- */

/**
@addtogroup grp_mf_mae

@brief Mean Absolute Error.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_mae_syntax">Syntax</a>
<li class="level1"><a href="#mf_mae_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_mae_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

@anchor mf_mae_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_mae(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The MAE value.

@anchor mf_mae_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

Rows containing NULLs are ignored.

@anchor mf_mae_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_mae(pred,obs) FROM test_set;
 mf_mae
--------
 13.825
(1 row)
\endcode

 */


-- begin of mf_mae definition

-- DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM AS (
    arg_sum FLOAT8,
    arg_num INTEGER
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mae_transition(
    oldval PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_sum+abs($2-$3),$1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mae_combine(
    old1 PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM,
    old2 PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM)
RETURNS PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM AS
$$
    SELECT ($1.arg_sum+$2.arg_sum,$1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mae_final(
    finalstate PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM)
RETURNS FLOAT8 AS
$$
    SELECT $1.arg_sum/$1.arg_num
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Mean Absolute Error.
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the mean absolute difference between them, this being
 * the Mean Absolute Error (MAE) prediction accuracy metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_mae(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The MAE value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_mae(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_mae_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_ARG_SUM_FLOAT8_AND_NUM,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_mae_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_mae_final,
    INITCOND='(0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mae()
RETURNS TEXT AS $$
SELECT $ABC$
mf_mae: Mean Absolute Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_mae('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mae(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_mae() ELSE $ABC$
mf_mae: Mean Absolute Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_mae(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the MAE value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute difference between them, this being
the Mean Absolute Error (MAE) prediction accuracy metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_mae(pred,obs) FROM test_set;
 mf_mae
--------
 13.825
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_mape

@brief Mean Absolute Percentage Error.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_mape_syntax">Syntax</a>
<li class="level1"><a href="#mf_mape_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_mape_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

@anchor mf_mape_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_mape(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The MAPE value.

@anchor mf_mape_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

Rows containing NULLs are ignored.

@anchor mf_mape_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_mape(pred,obs) FROM test_set;
      mf_mape      
-------------------
 0.294578793636013
(1 row)
\endcode

 */


-- begin of mf_mape definition

-- DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM AS (
    arg_sum FLOAT8,
    arg_num INTEGER
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mape_transition(
    oldval PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_sum+abs(($2-$3)/$3),$1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mape_combine(
    old1 PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM,
    old2 PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM)
RETURNS PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT ($1.arg_sum+$2.arg_sum,$1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mape_final(
    finalstate PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM)
RETURNS FLOAT8 AS
$$
    SELECT $1.arg_sum/$1.arg_num
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Mean Absolute Percentage Error.
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the mean absolute percentage difference between them,
 * this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
 * metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_mape(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The MAPE value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_mape(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_mape_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_MAPE_SUM_FLOAT8_AND_NUM,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_mape_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_mape_final,
    INITCOND='(0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mape()
RETURNS TEXT AS $$
SELECT $ABC$
mf_mape: Mean Absolute Percentage Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_mape('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mape(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_mape() ELSE $ABC$
mf_mape: Mean Absolute Percentage Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_mape(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the MAPE value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean absolute percentage difference between them,
this being the Mean Absolute Percentage Error (MAPE) prediction accuracy
metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_mape(pred,obs) FROM test_set;
      mf_mape      
-------------------
 0.294578793636013
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_mpe

@brief Mean Percentage Error.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_mpe_syntax">Syntax</a>
<li class="level1"><a href="#mf_mpe_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_mpe_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

@anchor mf_mpe_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_mpe(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The MPE value.

@anchor mf_mpe_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

@anchor mf_mpe_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_mpe(pred,obs) FROM test_set;
      mf_mpe       
-------------------
 -0.17248930032771
(1 row)
\endcode

 */


-- begin of mf_mpe definition

-- DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM AS (
    arg_sum FLOAT8,
    arg_num INTEGER
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mpe_transition(
    oldval PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_sum+(($2-$3)/$3),$1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mpe_combine(
    old1 PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM,
    old2 PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM)
RETURNS PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT ($1.arg_sum+$2.arg_sum,$1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_mpe_final(
    finalstate PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM)
RETURNS FLOAT8 AS
$$
    SELECT $1.arg_sum/$1.arg_num
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Mean Percentage Error.
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the mean percentage difference between them,
 * this being the Mean Percentage Error (MPE) prediction accuracy
 * metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_mpe(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The MPE value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_mpe(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_mpe_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_MPE_SUM_FLOAT8_AND_NUM,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_mpe_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_mpe_final,
    INITCOND='(0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mpe()
RETURNS TEXT AS $$
SELECT $ABC$
mf_mpe: Mean Percentage Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_mpe('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_mpe(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_mpe() ELSE $ABC$
mf_mpe: Mean Percentage Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

Rows containing NULLs are ignored.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_mpe(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the MPE value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the mean percentage difference between them,
this being the Mean Percentage Error (MPE) prediction accuracy
metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_mpe(pred,obs) FROM test_set;
      mf_mpe       
-------------------
 -0.17248930032771
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;


/**
@addtogroup grp_mf_rmse

@brief Root Mean Square Error.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_rmse_syntax">Syntax</a>
<li class="level1"><a href="#mf_rmse_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_rmse_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

@anchor mf_rmse_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_rmse(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The RMSE value.

@anchor mf_rmse_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

Rows containing NULLs are ignored.

@anchor mf_rmse_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_rmse(pred,obs) FROM test_set;
     mf_rmse      
------------------
 14.8442749907161
(1 row)
\endcode

 */


-- begin of mf_rmse definition

-- DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM AS (
    arg_sum FLOAT8,
    arg_num INTEGER
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_rmse_transition(
    oldval PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_sum+(($2-$3)^2),$1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_rmse_combine(
    old1 PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM,
    old2 PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM)
RETURNS PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM AS
$$
    SELECT ($1.arg_sum+$2.arg_sum,$1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_rmse_final(
    finalstate PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM)
RETURNS FLOAT8 AS
$$
    SELECT sqrt($1.arg_sum/$1.arg_num)
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Root Mean Square Error.
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the root mean square deviation between them,
 * this being the Root Mean Square Error (RMSE) prediction accuracy
 * metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_rmse(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The RMSE value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_rmse(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_rmse_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_RMSE_SUM_FLOAT8_AND_NUM,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_rmse_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_rmse_final,
    INITCOND='(0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_rmse()
RETURNS TEXT AS $$
SELECT $ABC$
mf_rmse: Root Mean Square Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_rmse('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_rmse(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_rmse() ELSE $ABC$
mf_rmse: Root Mean Square Error.

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_rmse(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the RMSE value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the root mean square deviation between them,
this being the Root Mean Square Error (RMSE) prediction accuracy
metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_rmse(pred,obs) FROM test_set;
     mf_rmse      
------------------
 14.8442749907161
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;


/**
@addtogroup grp_mf_r2

@brief R-squared.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_r2_syntax">Syntax</a>
<li class="level1"><a href="#mf_r2_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_r2_example">Example</a>
</ul>
</div>

@about
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

@anchor mf_r2_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_r2(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.

@returns The R2 value.

@anchor mf_r2_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

Rows containing NULLs are ignored.

@anchor mf_r2_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric.
\code
user=# SELECT mf_r2(pred,obs) FROM test_set;
       mf_r2       
-------------------
 0.279929088443373
(1 row)
\endcode

 */


-- begin of mf_r2 definition

-- DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8 CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8 AS (
    arg_yy FLOAT8, /* f=predicted, y=observed */
    arg_y FLOAT8,
    arg_fy FLOAT8,
    arg_ff FLOAT8,
    arg_num INTEGER
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_r2_transition(
    oldval PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8,
    newprediction FLOAT8,
    newobserved FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8 AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_yy+$3^2,$1.arg_y+$3,
                      $1.arg_fy+$2*$3,$1.arg_ff+$2^2,
                      $1.arg_num+1)::PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_r2_combine(
    old1 PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8,
    old2 PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8 AS
$$
    SELECT ($1.arg_yy+$2.arg_yy,$1.arg_y+$2.arg_y,
            $1.arg_fy+$2.arg_fy,$1.arg_ff+$2.arg_ff,
            $1.arg_num+$2.arg_num)::PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_r2_final(
    finalstate PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8)
RETURNS FLOAT8 AS
$$
    SELECT 1.0-($1.arg_yy-2*$1.arg_fy+$1.arg_ff)/
               ($1.arg_yy-$1.arg_y^2/$1.arg_num)
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Coefficient of determination (R^2)
 *
 * @about
 * An aggregate taking two columns, a "prediction" column and an "observed"
 * column and returning the coefficient of determination between them,
 * this being the R-squared (R^2) prediction accuracy metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_r2(prediction FLOAT8, observed FLOAT8);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 *
 * @returns The R^2 value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_r2(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_r2_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_R2_FLOAT8_FLOAT8,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_r2_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_r2_final,
    INITCOND='(0.0,0.0,0.0,0.0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_r2()
RETURNS TEXT AS $$
SELECT $ABC$
mf_r2: Coefficient of determination (R^2).

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_r2('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_r2(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_r2() ELSE $ABC$
mf_r2: Coefficient of determination (R^2).

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_r2(prediction FLOAT8, observed FLOAT8)
RETURNS FLOAT8


prediction - The column of predicted values.
observed   - The column of observed values.

Returns the R^2 value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column and returning the coefficient of determination between them,
this being the R-squared (R^2) prediction accuracy metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric.

user=# SELECT PDLTOOLS_SCHEMA.mf_r2(pred,obs) FROM test_set;
       mf_r2       
-------------------
 0.279929088443373
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_adjusted_r2

@brief Adjusted R-squared.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_adjusted_r2_syntax">Syntax</a>
<li class="level1"><a href="#mf_adjusted_r2_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_adjusted_r2_example">Example</a>
</ul>
</div>

@about
An aggregate calculating the adjusted R-squared prediction accuracy
metric.

@anchor mf_adjusted_r2_syntax
@par Syntax
<pre class="syntax">
AGGREGATE mf_adjusted_r2(prediction FLOAT8, observed FLOAT8,
                         num_predictors INTEGER, training_size INTEGER)
RETURNS FLOAT8
</pre>

@param prediction The column of predicted values.
@param observed The column of observed values.
@param num_predictors The number of parameters in the predicting model, not counting the constant term. Should be an integer constant.
@param training_size The number of rows used for training, excluding any NULL rows. Should be an integer constant.

@returns The adjusted R2 value.

@anchor mf_adjusted_r2_usage
@usage
An aggregate taking two columns, a "prediction" column and an "observed"
column, as well as two integers describing the degrees of freedom of the model
and the size of the training set over which it was developed, and returning
the adjusted R-squared prediction accuracy metric.

Rows containing NULLs are ignored.

@anchor mf_adjusted_r2_example
@examp

Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

\code
user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4
\endcode
\n
Now, we use the aggregate to calculate the metric. The parameter '3' indicates
that other than the constant term, the model has three parameters (e.g., it
may take the form 7+5x+39y+0.91z). The parameter '100' indicates that the
model was trained on a traininst set with 100 rows (excluding any NULL rows).
Neither of these parameters can be deduced from the predicted values and
the test set data alone.
\code
user=# SELECT mf_adjusted_r2(pred,obs,3,100) FROM test_set;
  mf_adjusted_r2   
-------------------
 0.257426872457228
(1 row)
\endcode

 */


-- begin of mf_adjusted_r2 definition

-- DROP TYPE IF EXISTS PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8 CASCADE;

CREATE TYPE PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8 AS (
    arg_yy FLOAT8, /* f=predicted, y=observed */
    arg_y FLOAT8,
    arg_fy FLOAT8,
    arg_ff FLOAT8,
    arg_num INTEGER,
    arg_p INTEGER, /* p=parameters, n=training size */
    arg_n INTEGER
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_transition(
    oldval PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8,
    newprediction FLOAT8,
    newobserved FLOAT8,
    newp INTEGER,
    newn INTEGER)
RETURNS PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8 AS
$$
    SELECT CASE WHEN $2 IS NULL OR $3 IS NULL THEN $1
                ELSE ($1.arg_yy+$3^2,$1.arg_y+$3,
                      $1.arg_fy+$2*$3,$1.arg_ff+$2^2,
                      $1.arg_num+1,
                      greatest($1.arg_p,$4),
                      greatest($1.arg_n,$5))::PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8
           END
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_combine(
    old1 PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8,
    old2 PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8)
RETURNS PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8 AS
$$
    SELECT ($1.arg_yy+$2.arg_yy,$1.arg_y+$2.arg_y,
            $1.arg_fy+$2.arg_fy,$1.arg_ff+$2.arg_ff,
            $1.arg_num+$2.arg_num,
            greatest($1.arg_p,$2.arg_p),
            greatest($1.arg_n,$2.arg_n))::PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8
$$
LANGUAGE sql IMMUTABLE;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_final(
    finalstate PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8)
RETURNS FLOAT8 AS
$$
    SELECT 1.0-($1.arg_yy-2*$1.arg_fy+$1.arg_ff)*($1.arg_n-1)/
               (($1.arg_yy-$1.arg_y^2/$1.arg_num)*($1.arg_n-$1.arg_p-1))
$$
LANGUAGE sql IMMUTABLE;

/**
 * @brief Adjusted R^2
 *
 * @about
 * An aggregate calculating the Adjusted R-squared prediction accuracy metric.
 *
 * @par Syntax
 * <pre class="syntax">
 * AGGREGATE mf_adjusted_r2(prediction FLOAT8, observed FLOAT8,
 *                          num_predictors INTEGER, training_size INTEGER);
 * </pre>
 *
 * @param prediction The column of predicted values.
 * @param observed The column of observed values.
 * @param num_predictors The number of parameters in the predicting model, not counting the constant term. Should be an integer constant.
 * @param training_size The number of rows used for training, excluding any NULL rows. Should be an integer constant.
 *
 * @returns The adjusted R^2 value.
 *
 */

CREATE AGGREGATE PDLTOOLS_SCHEMA.mf_adjusted_r2(/*+ prediction */ FLOAT8,
                                        /*+ observed */ FLOAT8,
                                        /*+ num_predictors */ INTEGER,
                                        /*+ training_size */ INTEGER) (
    SFUNC=PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_transition,
    STYPE=PDLTOOLS_SCHEMA.__PM_ADJUSTED_R2_FLOAT8_FLOAT8,
    PREFUNC=PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_combine,
    FINALFUNC=PDLTOOLS_SCHEMA.__pm_mf_adjusted_r2_final,
    INITCOND='(0.0,0.0,0.0,0.0,0,0,0)'
);

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_adjusted_r2()
RETURNS TEXT AS $$
SELECT $ABC$
mf_adjusted_r2: Adjusted R^2.

An aggregate calculating the adjusted R-squared prediction accuracy metric.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_adjusted_r2('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_adjusted_r2(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_adjusted_r2() ELSE $ABC$
mf_adjusted_r2: Adjusted R^2.

An aggregate taking two columns, a "prediction" column and an "observed"
column, as well as two integers describing the degrees of freedom of the model
and the size of the training set over which it was developed, and returning
the adjusted R-squared prediction accuracy metric.

Syntax
======
AGGREGATE PDLTOOLS_SCHEMA.mf_adjusted_r2(prediction FLOAT8, observed FLOAT8,
                                  num_predictors INTEGER, trainig_size INTEGER)
RETURNS FLOAT8


prediction     - The column of predicted values.
observed       - The column of observed values.
num_predictors - The number of parameters in the predicting model, not
                 counting the constant term. Should be an integer constant.
training_size  - The number of rows used for training, excluding any NULL
                 rows. Should be an integer constant.

Returns the adjusted R^2 value.

Usage
=====

An aggregate taking two columns, a "prediction" column and an "observed"
column, as well as two integers describing the degrees of freedom of the model
and the size of the training set over which it was developed, and returning
the adjusted R-squared prediction accuracy metric.

Rows containing NULLs are ignored.

Example
=======
Assume we have trained a model and used it to reach predictions on certain
values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we create the table manually.

user=# CREATE TABLE test_set(
user(#                       pred FLOAT8,
user(#                       obs FLOAT8
user(#                      ) DISTRIBUTED RANDOMLY;
CREATE TABLE
user=# INSERT INTO test_set VALUES
user-#   (37.5,53.1),(12.3,34.2), (74.2,65.4), (91.1,82.1);
INSERT 0 4

Now, we use the aggregate to calculate the metric. The parameter '3' indicates
that other than the constant term, the model has three parameters (e.g., it
may take the form 7+5x+39y+0.91z). The parameter '100' indicates that the
model was trained on a traininst set with 100 rows (excluding any NULL rows).
Neither of these parameters can be deduced from the predicted values and
the test set data alone.

user=# SELECT PDLTOOLS_SCHEMA.mf_adjusted_r2(pred,obs,3,100) FROM test_set;
  mf_adjusted_r2   
-------------------
 0.257426872457228
(1 row)
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_binary_classifier

@brief Metrics for binary classification.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_binary_classifier_syntax">Syntax</a>
<li class="level1"><a href="#mf_binary_classifier_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_binary_classifier_example">Example</a>
</ul>
</div>

@about
A function calculating a collection of prediction metrics relevant for
binary classification.

@anchor mf_binary_classifier_syntax
@par Syntax
<pre class="syntax">
FUNCTION mf_binary_classifier(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT
                              [, grouping_col TEXT])
RETURNS VOID
</pre>

@param in_table Name of the input table.
@param prediction_col Name of the column of predicted values.
       (These are numeric values corresponding to
       likelihood/probability: a larger value corresponds to greater
       certainty that the observed value will be '1', lower value
       corresponds to a greater certainty that it will be '0'.)
@param observed_col Name of the column of observed values. (A 0/1 column.)
@param out_table Name of the output table.
@param grouping_col Optional argument, indicating which column(s) to group by.
                    If grouping by multiple columns, these should be passed
                    as a comma-separated list.

@anchor mf_binary_classifier_usage
@usage
A function calculating the true positive count (TP), true negative count (TN),
false positive count (FP), false negative count (FN),
true positive rate [=sensitivity = hit rate = recall] (TPR),
true negative rate [=specificity] (TNR),
positive predictive value [=precision] (PPV),
negative predictive value (NPV), false positive rate [=fall-out] (FPR),
false discovery rate (FDR), false negative rate [=miss rate] (FNR),
accuracy (ACC) and F1 score (F1) metrics, each of which is calculated for
every mapping of the prediction column values into 0/1 choices based on a
threshold value. The maximum threshold value resulting in each such mapping is
given in the output column \c 'threshold' in the output table. Each metric
is detailed in its own column in the same table. If grouping columns are
specified, they appear in the output as additional columns, with their
original names.

The definitions of the various metrics are as follows.

- \f$\textit{tp}\f$ is the count of correctly-classified positives.
- \f$\textit{tn}\f$ is the count of correctly-classified negatives.
- \f$\textit{fp}\f$ is the count of misclassified negatives.
- \f$\textit{fn}\f$ is the count of misclassified positives.
- \f$\textit{tpr}=\textit{tp}/(\textit{tp}+\textit{fn})\f$.
- \f$\textit{tnr}=\textit{tn}/(\textit{fp}+\textit{tn})\f$.
- \f$\textit{ppv}=\textit{tp}/(\textit{tp}+\textit{fp})\f$.
- \f$\textit{npv}=\textit{tn}/(\textit{tn}+\textit{fn})\f$.
- \f$\textit{fpr}=\textit{fp}/(\textit{fp}+\textit{tn})\f$.
- \f$\textit{fdr}=1-\textit{ppv}\f$.
- \f$\textit{fnr}=\textit{fn}/(\textit{fn}+\textit{tp})\f$.
- \f$\textit{acc}=(\textit{tp}+\textit{tn})/(\textit{tp}+\textit{tn}+\textit{fp}+\textit{fn})\f$.
- \f$\textit{f1}=2*\textit{tp}/(2*\textit{tp}+\textit{fp}+\textit{fn})\f$.

Rows with NULL observations are ignored in the metric calculation. The
 predicted value should not be NULL.

@anchor mf_binary_classifier_example
@examp

Assume we have trained a model and used it to perform binary classification on
certain values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

The observed value is a 0/1 binary classification, and the predicted value
is a number, corresponding to the probability/likelihood of a '1'. For example,
such a value may be the resulto of logistic regression.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we will create the following dummy test_set table:

\code
user=# CREATE TABLE test_set AS
user-#   SELECT ((a*8)::integer)/8.0 pred,
user-#          ((a*0.5+random()*0.5)>0.5)::integer obs
user-#   FROM (select random() as a from generate_series(1,100)) x
user-# DISTRIBUTED RANDOMLY;
\endcode

In this toy example, "obs" takes the place of the observed parameter, and
"pred" takes the place of its approximation by a model. We are interested
in determining the quality of this approximation at the various possible
decision thresholds.

\code
user=# SELECT mf_binary_classifier('test_set','pred','obs','out_table');
 mf_binary_classifier   
----------------------
(1 row)
\endcode

Suppose, now, that we are interested in observing the True Positive Rate and
the False Positive Rate. They can be retrieved as follows.

\code
user=# SELECT threshold, tpr, fpr FROM out_table ORDER BY threshold;
       threshold        |          tpr           |          fpr           
------------------------+------------------------+------------------------
 0.00000000000000000000 | 1.00000000000000000000 | 1.00000000000000000000
 0.12500000000000000000 | 1.00000000000000000000 | 0.91228070175438596491
 0.25000000000000000000 | 0.97674418604651162791 | 0.70175438596491228070
 0.37500000000000000000 | 0.83720930232558139535 | 0.42105263157894736842
 0.50000000000000000000 | 0.72093023255813953488 | 0.29824561403508771930
 0.62500000000000000000 | 0.60465116279069767442 | 0.15789473684210526316
 0.75000000000000000000 | 0.48837209302325581395 | 0.07017543859649122807
 0.87500000000000000000 | 0.30232558139534883721 | 0.00000000000000000000
 1.00000000000000000000 | 0.09302325581395348837 | 0.00000000000000000000
(9 rows)
\endcode

Alternatively, we can retrieve all metrics at a given threshold value.

\code
user=# \x
Expanded display is on.
user=# SELECT * FROM out_table WHERE threshold=0.5;
-[ RECORD 1 ]---------------------
threshold | 0.50000000000000000000
tp        | 31
fp        | 17
fn        | 12
tn        | 40
tpr       | 0.72093023255813953488
tnr       | 0.70175438596491228070
ppv       | 0.64583333333333333333
npv       | 0.76923076923076923077
fpr       | 0.29824561403508771930
fdr       | 0.35416666666666666667
fnr       | 0.27906976744186046512
acc       | 0.71000000000000000000
f1        | 0.68131868131868131868

michaeldb=# \x
Expanded display is off.
\endcode

The same analysis can be performed under grouping, including multi-column
grouping.

@sa grp_mf_auc

 */


CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT)
RETURNS VOID
VOLATILE
STRICT
LANGUAGE PLPythonU
AS
$$
  params=dict(in_table=in_table,prediction_col=prediction_col,
              observed_col=observed_col, out_table=out_table)
  plpy.execute("""
    CREATE TABLE {out_table} AS SELECT *,
      tp*1.0/NULLIF(tp+fn,0) AS tpr,
      tn*1.0/NULLIF(fp+tn,0) AS tnr,
      tp*1.0/NULLIF(tp+fp,0) AS ppv,
      tn*1.0/NULLIF(tn+fn,0) AS npv,
      fp*1.0/NULLIF(fp+tn,0) AS fpr,
      fp*1.0/NULLIF(fp+tp,0) AS fdr,
      fn*1.0/NULLIF(fn+tp,0) AS fnr,
      (tp+tn)*1.0/NULLIF(tp+tn+fp+fn,0) AS acc,
      2.0*tp/NULLIF(2*tp+fp+fn,0) AS f1
    FROM ( 
      SELECT threshold,
             sum(t) OVER (ORDER BY threshold DESC) AS tp,
             sum(f) OVER (ORDER BY threshold DESC) AS fp,
             sum(t) OVER () - sum(t) OVER (ORDER BY threshold DESC) AS fn,
             sum(f) OVER () - sum(f) OVER (ORDER BY threshold DESC) AS tn
        FROM (
          SELECT {prediction_col} AS threshold,sum({observed_col}) AS t,
                 count(*)-sum({observed_col}) AS f
            FROM {in_table}
          GROUP BY (threshold)
        ) x
    ) y
    ORDER BY threshold
    DISTRIBUTED RANDOMLY;
  """.format(**params))
$$;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT,
                              grouping_col TEXT)
RETURNS VOID
VOLATILE
STRICT
LANGUAGE PLPythonU
AS
$$
  params=dict(in_table=in_table,prediction_col=prediction_col,
              observed_col=observed_col,
              out_table=out_table, grouping_col=grouping_col)
  plpy.execute("""
    CREATE TABLE {out_table} AS SELECT *,
      tp*1.0/NULLIF(tp+fn,0) AS tpr,
      tn*1.0/NULLIF(fp+tn,0) AS tnr,
      tp*1.0/NULLIF(tp+fp,0) AS ppv,
      tn*1.0/NULLIF(tn+fn,0) AS npv,
      fp*1.0/NULLIF(fp+tn,0) AS fpr,
      fp*1.0/NULLIF(fp+tp,0) AS fdr,
      fn*1.0/NULLIF(fn+tp,0) AS fnr,
      (tp+tn)*1.0/NULLIF(tp+tn+fp+fn,0) AS acc,
      2.0*tp/NULLIF(2*tp+fp+fn,0) AS f1
    FROM ( 
      SELECT {grouping_col},threshold,
             sum(t) OVER (PARTITION BY {grouping_col}
                          ORDER BY threshold DESC) AS tp,
             sum(f) OVER (PARTITION BY {grouping_col}
                          ORDER BY threshold DESC) AS fp,
             sum(t) OVER (PARTITION BY {grouping_col})
               - sum(t) OVER (PARTITION BY {grouping_col}
                              ORDER BY threshold DESC) AS fn,
             sum(f) OVER (PARTITION BY {grouping_col})
               - sum(f) OVER (PARTITION BY {grouping_col}
                              ORDER BY threshold DESC) AS tn
        FROM (
          SELECT {grouping_col},
                 {prediction_col} AS threshold,
                 sum({observed_col}) AS t,
                 count(*)-sum({observed_col}) AS f
            FROM {in_table}
          GROUP BY ({grouping_col}, threshold)
        ) x
    ) y
    DISTRIBUTED BY ({grouping_col});
  """.format(**params))
$$;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier()
RETURNS TEXT AS $$
SELECT $ABC$
mf_binary_classifier: Metrics for binary classification.

A function calculating a collection of prediction metrics relevant for
binary classification.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_binary_classifier('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_binary_classifier() ELSE $ABC$
mf_binary_classifier: Metrics for binary classification.

A function calculating a collection of prediction metrics relevant for
binary classification.

Syntax
======
FUNCTION PDLTOOLS_SCHEMA.mf_binary_classifier(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT
                              [, grouping_col TEXT])
RETURNS VOID

in_table       - Name of the input table.
prediction_col - Name of the column of predicted values.
                 (These are numeric values corresponding to
                 likelihood/probability: a larger value corresponds to greater
                 certainty that the observed value will be '1', lower value
                 corresponds to a greater certainty that it will be '0'.)
observed_col   - Name of the column of observed values. (A 0/1 column.)
out_table      - Name of the output table.
grouping_col   - Optional argument, indicating which column(s) to group by.
                    If grouping by multiple columns, these should be passed
                    as a comma-separated list.

Usage
=====

A function calculating the true positive count (TP), true negative count (TN),
false positive count (FP), false negative count (FN),
true positive rate [=sensitivity = hit rate = recall] (TPR),
true negative rate [=specificity] (TNR),
positive predictive value [=precision] (PPV),
negative predictive value (NPV), false positive rate [=fall-out] (FPR),
false discovery rate (FDR), false negative rate [=miss rate] (FNR),
accuracy (ACC) and F1 score (F1) metrics, each of which is calculated for
every mapping of the prediction column values into 0/1 choices based on a
threshold value. The maximum threshold value resulting in each such mapping is
given in the output column 'threshold' in the output table. Each metric
is detailed in its own column in the same table. If grouping columns are
specified, they appear in the output as additional columns, with their
original names.

The definitions of the various metrics are as follows.

- tp is the count of correctly-classified positives.
- tn is the count of correctly-classified negatives.
- fp is the count of misclassified negatives.
- fn is the count of misclassified positives.
- tpr=tp/(tp+fn).
- tnr=tn/(fp+tn).
- ppv=tp/(tp+fp).
- npv=tn/(tn+fn).
- fpr=fp/(fp+tn).
- fdr=1-ppv.
- fnr=fn/(fn+tp).
- acc=(tp+tn)/(tp+tn+fp+fn).
- f1=2*tp/(2*tp+fp+fn).

Rows with NULL observations are ignored in the metric calculation. The
 predicted value should not be NULL.

Example
=======
Assume we have trained a model and used it to perform binary classification on
certain values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

The observed value is a 0/1 binary classification, and the predicted value
is a number, corresponding to the probability/likelihood of a '1'. For example,
such a value may be the resulto of logistic regression.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we will create the following dummy test_set table:

user=# CREATE TABLE test_set AS
user-#   SELECT ((a*8)::integer)/8.0 pred,
user-#          ((a*0.5+random()*0.5)>0.5)::integer obs
user-#   FROM (select random() as a from generate_series(1,100)) x
user-# DISTRIBUTED RANDOMLY;

In this toy example, "obs" takes the place of the observed parameter, and
"pred" takes the place of its approximation by a model. We are interested
in determining the quality of this approximation at the various possible
decision thresholds.

user=# SELECT mf_binary_classifier('test_set','pred','obs','out_table');
 mf_binary_classifier   
----------------------
(1 row)

Suppose, now, that we are interested in observing the True Positive Rate and
the False Positive Rate. They can be retrieved as follows.

user=# SELECT threshold, tpr, fpr FROM out_table ORDER BY threshold;
       threshold        |          tpr           |          fpr           
------------------------+------------------------+------------------------
 0.00000000000000000000 | 1.00000000000000000000 | 1.00000000000000000000
 0.12500000000000000000 | 1.00000000000000000000 | 0.91228070175438596491
 0.25000000000000000000 | 0.97674418604651162791 | 0.70175438596491228070
 0.37500000000000000000 | 0.83720930232558139535 | 0.42105263157894736842
 0.50000000000000000000 | 0.72093023255813953488 | 0.29824561403508771930
 0.62500000000000000000 | 0.60465116279069767442 | 0.15789473684210526316
 0.75000000000000000000 | 0.48837209302325581395 | 0.07017543859649122807
 0.87500000000000000000 | 0.30232558139534883721 | 0.00000000000000000000
 1.00000000000000000000 | 0.09302325581395348837 | 0.00000000000000000000
(9 rows)

Alternatively, we can retrieve all metrics at a given threshold value.

user=# \x
Expanded display is on.
user=# SELECT * FROM out_table WHERE threshold=0.5;
-[ RECORD 1 ]---------------------
threshold | 0.50000000000000000000
tp        | 31
fp        | 17
fn        | 12
tn        | 40
tpr       | 0.72093023255813953488
tnr       | 0.70175438596491228070
ppv       | 0.64583333333333333333
npv       | 0.76923076923076923077
fpr       | 0.29824561403508771930
fdr       | 0.35416666666666666667
fnr       | 0.27906976744186046512
acc       | 0.71000000000000000000
f1        | 0.68131868131868131868

michaeldb=# \x
Expanded display is off.

The same analysis can be performed under grouping, including multi-column
grouping.
$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_auc

@brief Area under the ROC curve (in binary classification).

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_auc_syntax">Syntax</a>
<li class="level1"><a href="#mf_auc_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_auc_example">Example</a>
</ul>
</div>

@about
A function calculating the area under the ROC curve, for use in
binary classification.

@anchor mf_auc_syntax
@par Syntax
<pre class="syntax">
FUNCTION mf_auc(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT)
RETURNS DOUBLE PRECISION
</pre>
<pre class="syntax">
FUNCTION mf_auc(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT,
                              grouping_col TEXT)
RETURNS VOID
</pre>

@param in_table Name of the input table.
@param prediction_col Name of the column of predicted values.
       (These are numeric values corresponding to
       likelihood/probability: a larger value corresponds to greater
       certainty that the observed value will be '1', lower value
       corresponds to a greater certainty that it will be '0'.)
@param observed_col Name of the column of observed values. (A 0/1 column.)
@param out_table Name of the output table.
@param grouping_col Argument indicating which column(s) to group by.
                    If grouping by multiple columns, these should be passed
                    as a comma-separated list.

@anchor mf_auc_usage
@usage
A function calculating the area under the Receiver Operating Characteristic
curve for binary classification (the AUC). Without grouping, this is a number
and is returned immediately as the functon's return value. With grouping,
the answer is different for each group, and is given in the table specified
in \a 'output_table'. The grouping columns appear in the output as additional
columns, with their original names.

The ROC curve is the curve relating the classifier's TPR and FPR metrics.
(See \ref grp_mf_binary_classifier for a definition of these metrics.)

Rows with NULL observations are ignored in the calculation. The
 predicted value should not be NULL.

@anchor mf_auc_example
@examp
Assume we have trained a model and used it to perform binary classification on
certain values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

The observed value is a 0/1 binary classification, and the predicted value
is a number, corresponding to the probability/likelihood of a '1'. For example,
such a value may be the resulto of logistic regression.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we will create the following dummy test_set table:

\code
user=# CREATE TABLE test_set AS
user-#   SELECT ((a*8)::integer)/8.0 pred,
user-#          ((a*0.5+random()*0.5)>0.5)::integer obs,
user-#          (random()>0.5)::integer m1,
user-#          (random()>0.5)::integer m2
user-#   FROM (select random() as a from generate_series(1,100)) x
user-# DISTRIBUTED RANDOMLY;
\endcode

In this toy example, "obs" takes the place of the observed parameter, and
"pred" takes the place of its approximation by a model. We are interested
in determining the quality of this approximation.

\code
user=# SELECT mf_auc('test_set','pred','obs');
     mf_auc     
----------------
 0.814769481844
(1 row)
\endcode

Suppose, now, that we are interested in observing this quality metric
separately for each group defined by the values of \c 'm1' and \c 'm2'.

\code
user=# SELECT pdltools.mf_auc('test_set','pred','obs','out_table','m1, m2');
 mf_auc 
--------
 
(1 row)

michaeldb=# SELECT * from out_table;
 m1 | m2 |                     auc                     
----+----+---------------------------------------------
  0 |  1 | 0.70614035087719298245804824561403508771930
  1 |  0 | 0.93000000000000000000000000000000000000000
  0 |  0 | 0.87797619047619047619250000000000000000000
  1 |  1 | 0.78968253968253968254051587301587301587305
(4 rows)
\endcode

@sa grp_mf_binary_classifier

 */
CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_auc(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT)
RETURNS DOUBLE PRECISION
STABLE
STRICT
LANGUAGE PLPythonU
AS
$$
  params=dict(in_table=in_table,prediction_col=prediction_col,
              observed_col=observed_col)
  return plpy.execute("""
    SELECT sum((tpr+prev_tpr)*(fpr-prev_fpr)*0.5) AS auc
    FROM (
      SELECT tpr, fpr,
             coalesce(lag(tpr) OVER (ORDER BY threshold DESC),0) AS prev_tpr,
             coalesce(lag(fpr) OVER (ORDER BY threshold DESC),0) AS prev_fpr
      FROM(
        SELECT threshold,
               sum(t) OVER (ORDER BY threshold DESC)
                 *1.0/(sum(t) OVER ()) AS tpr,
               sum(f) OVER (ORDER BY threshold DESC)
                 *1.0/(sum(f) OVER ()) AS fpr
        FROM (
          SELECT {prediction_col} AS threshold,sum({observed_col}) AS t,
                 count(*)-sum({observed_col}) AS f
          FROM {in_table}
          GROUP BY (threshold)
        ) x
      ) y
    ) z;
  """.format(**params))[0]['auc']
$$;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_auc(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT, grouping_col TEXT)
RETURNS VOID
VOLATILE
STRICT
LANGUAGE PLPythonU
AS
$$
  params=dict(in_table=in_table,prediction_col=prediction_col,
              observed_col=observed_col,
              out_table=out_table, grouping_col=grouping_col)
  plpy.execute("""
    CREATE TABLE {out_table} AS
    SELECT {grouping_col}, sum((tpr+prev_tpr)*(fpr-prev_fpr)*0.5) AS auc
    FROM (
      SELECT {grouping_col}, tpr, fpr,
             coalesce(lag(tpr) OVER (PARTITION BY {grouping_col}
                                     ORDER BY threshold DESC),0) AS prev_tpr,
             coalesce(lag(fpr) OVER (PARTITION BY {grouping_col}
                                     ORDER BY threshold DESC),0) AS prev_fpr
      FROM(
        SELECT {grouping_col}, threshold,
               sum(t) OVER (PARTITION BY {grouping_col}
                            ORDER BY threshold DESC)
                 *1.0/(sum(t) OVER (PARTITION BY {grouping_col})) AS tpr,
               sum(f) OVER (PARTITION BY {grouping_col}
                            ORDER BY threshold DESC)
                 *1.0/(sum(f) OVER (PARTITION BY {grouping_col})) AS fpr
        FROM (
          SELECT {grouping_col},
                 {prediction_col} AS threshold,sum({observed_col}) AS t,
                 count(*)-sum({observed_col}) AS f
          FROM {in_table}
          GROUP BY ({grouping_col},threshold)
        ) x
      ) y
    ) z
    GROUP BY ({grouping_col})
    DISTRIBUTED BY ({grouping_col});
  """.format(**params))
$$;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_auc()
RETURNS TEXT AS $$
SELECT $ABC$
mf_auc: Area under the ROC curve (in binary classification).

A function calculating the area under the ROC curve, for use in
binary classification.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_auc('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_auc(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_auc() ELSE $ABC$
mf_auc: Area under the ROC curve (in binary classification).

A function calculating the area under the ROC curve, for use in
binary classification.

Syntax
======
FUNCTION PDLTOOLS_SCHEMA.mf_auc(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT)
RETURNS DOUBLE PRECISION

FUNCTION PDLTOOLS_SCHEMA.mf_auc(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT,
                              grouping_col TEXT)
RETURNS VOID

in_table       - Name of the input table.
prediction_col - Name of the column of predicted values.
                 (These are numeric values corresponding to
                 likelihood/probability: a larger value corresponds to greater
                 certainty that the observed value will be '1', lower value
                 corresponds to a greater certainty that it will be '0'.)
observed_col   - Name of the column of observed values. (A 0/1 column.)
out_table      - Name of the output table.
grouping_col   - Optional argument, indicating which column(s) to group by.
                    If grouping by multiple columns, these should be passed
                    as a comma-separated list.

Usage
=====
A function calculating the area under the Receiver Operating Characteristic
curve for binary classification (the AUC). Without grouping, this is a number
and is returned immediately as the functon's return value. With grouping,
the answer is different for each group, and is given in the table specified
in 'output_table'. The grouping columns appear in the output as additional
columns, with their original names.

The ROC curve is the curve relating the classifier's TPR and FPR metrics.
(See "mf_binary_classifier()" for a definition of these metrics.)

Rows with NULL observations are ignored in the calculation. The
 predicted value should not be NULL.

Example
=======
Assume we have trained a model and used it to perform binary classification on
certain values of the independent parameters. (In the example, these
values are taken to be from a "test_set", implying that they are
distinct from those values on which the model was trained. However,
in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

The observed value is a 0/1 binary classification, and the predicted value
is a number, corresponding to the probability/likelihood of a '1'. For example,
such a value may be the resulto of logistic regression.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we will create the following dummy test_set table:

user=# CREATE TABLE test_set AS
user-#   SELECT ((a*8)::integer)/8.0 pred,
user-#          ((a*0.5+random()*0.5)>0.5)::integer obs,
user-#          (random()>0.5)::integer m1,
user-#          (random()>0.5)::integer m2
user-#   FROM (select random() as a from generate_series(1,100)) x
user-# DISTRIBUTED RANDOMLY;

In this toy example, "obs" takes the place of the observed parameter, and
"pred" takes the place of its approximation by a model. We are interested
in determining the quality of this approximation.

user=# SELECT mf_auc('test_set','pred','obs');
     mf_auc     
----------------
 0.814769481844
(1 row)

Suppose, now, that we are interested in observing this quality metric
separately for each group defined by the values of 'm1' and 'm2'.

user=# SELECT pdltools.mf_auc('test_set','pred','obs','out_table','m1, m2');
 mf_auc 
--------
 
(1 row)

michaeldb=# SELECT * from out_table;
 m1 | m2 |                     auc                     
----+----+---------------------------------------------
  0 |  1 | 0.70614035087719298245804824561403508771930
  1 |  0 | 0.93000000000000000000000000000000000000000
  0 |  0 | 0.87797619047619047619250000000000000000000
  1 |  1 | 0.78968253968253968254051587301587301587305
(4 rows)

See also: mf_binary_classifier

$ABC$::TEXT
END;
$$ LANGUAGE SQL;

/**
@addtogroup grp_mf_confusion_matrix

@brief Confusion matrix for a multi-class classifier.

<div class="toc"><b>Contents</b>
<ul>
<li class="level1"><a href="#mf_confusion_matrix_syntax">Syntax</a>
<li class="level1"><a href="#mf_confusion_matrix_linkage_usage">Usage</a>
<li class="level1"><a href="#mf_confusion_matrix_example">Example</a>
</ul>
</div>

@about
A function calculating the confusion matrix, for use in
multi-class classification.

@anchor mf_confusion_matrix_syntax
@par Syntax
<pre class="syntax">
FUNCTION mf_confusion_matrix(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT)
RETURNS VOID
</pre>

@param in_table Name of the input table.
@param prediction_col Name of the column of predicted values.
@param observed_col Name of the column of observed values.
@param out_table Name of the output table.

@anchor mf_confusion_matrix_usage
@usage
In multi-class classification, observations and predictions are taken from
some finite set. Elements of this set may be encoded in any form, e.g. as
numbers or strings. Each element is known as a 'class'.
A confusion matrix, which is the output of this function, details for each
\c 'x' and \c 'y', how many times in the data given in \a 'in_table', a
prediction of \c 'x' corresponded to an observed actual value of \c 'y'.
The function can perform this regardless of the type of data in the
prediction and observed columns, but this type should be the same in both.

The output is a table with two columns:
 - \a 'class' - The observed class.
 - \a 'confusion_arr' - an array of integers, each integer corresponding to
                        the number of times a particular predicted value
                        corresponded to the observed class. The array is
                        ordered by the predicted value.

The \a 'confusion_arr' rows, when ordered by the observed class, form a
square matrix whose diagonal corresponds to 'correct' classifications. The
matrix includes rows (and columns) for each class represented in
\a 'in_table', whether it was seen as an observed or as a predicted class.

@anchor mf_confusion_matrix_example
@examp
Assume we have trained a model and used it to perform multi-class
classification on certain values of the independent parameters.
(In the example, these values are taken to be from a "test_set", implying
that they are distinct from those values on which the model was trained.
However, in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

In the example, the classes are the numbers 0 through 5. However, to
demonstrate how the UDF functions when there is incomplete overlap between
predicted and observed values, we will only use 1 through 5 for predictions
and only use 0 through 4 for observed values.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we will create the following dummy test_set table:

\code
user=$ CREATE TABLE test_set AS
user-$   SELECT (x+y)%5+1 AS pred,
user-$          (x*y)%5 AS obs
user-$    FROM generate_series(1,5) x,
user-$         generate_series(1,5) y
user-$ DISTRIBUTED RANDOMLY;
\endcode

The confusion matrix is generated as follows.

\code
user=# SELECT mf_confusion_matrix('test_set','pred','obs','out_table');
 mf_confusion_matrix 
---------------------
 
(1 row)

michaeldb=# SELECT * FROM out_table ORDER BY class;
 class | confusion_arr 
-------+---------------
     0 | {0,1,2,2,2,2}
     1 | {0,2,0,1,1,0}
     2 | {0,0,0,2,2,0}
     3 | {0,0,2,0,0,2}
     4 | {0,2,1,0,0,1}
     5 | {0,0,0,0,0,0}
(6 rows)
\endcode

As can be seen, the generated matrix contains rows and columns for all values
between 0 and 5, even though some were never seen as observed classes and
others were never seen as predicted classes.

 */
CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_confusion_matrix(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT)
RETURNS VOID
VOLATILE
STRICT
LANGUAGE PLPythonU
AS
$$
  params=dict(in_table=in_table,prediction_col=prediction_col,
              observed_col=observed_col,out_table=out_table)
  plpy.execute("""
    CREATE TABLE {out_table} AS
      SELECT class,
             array_agg(cnt ORDER BY pred) AS confusion_arr
      FROM (
        SELECT class,
               pred,
               sum(cnt) AS cnt
        FROM (
          SELECT {observed_col} AS class,
                 {prediction_col} AS pred,
                 count(*) AS cnt
          FROM {in_table}
          GROUP BY (class, pred) 
          UNION
          SELECT a, b, 0
          FROM (
            SELECT {observed_col} AS a
            FROM {in_table}
            GROUP BY (a)
            UNION
            SELECT {prediction_col}
            FROM {in_table}
            GROUP BY ({prediction_col})
          ) r,
          ( SELECT {observed_col} AS b
            FROM {in_table}
            GROUP BY (b)
            UNION
            SELECT {prediction_col}
            FROM {in_table}
            GROUP BY ({prediction_col})
          ) s
        ) y
        GROUP BY (class, pred) 
      ) x
      GROUP BY class
    DISTRIBUTED BY (class);
  """.format(**params))
$$;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_confusion_matrix()
RETURNS TEXT AS $$
SELECT $ABC$
mf_confusion_matrix: Confusion matrix for a multi-class classifier.

A function calculating the confusion matrix, for use in
multi-class classification.

For full usage instructions, run "PDLTOOLS_SCHEMA.mf_confusion_matrix('usage')".
$ABC$::TEXT;
$$ LANGUAGE SQL;

CREATE OR REPLACE FUNCTION PDLTOOLS_SCHEMA.mf_confusion_matrix(TEXT)
RETURNS TEXT AS $$
SELECT CASE WHEN $1!='usage' THEN PDLTOOLS_SCHEMA.mf_confusion_matrix() ELSE $ABC$
mf_confusion_matrix: Confusion matrix for a multi-class classifier.

A function calculating the confusion matrix, for use in
multi-class classification.

Syntax
======
FUNCTION PDLTOOLS_SCHEMA.mf_confusion_matrix(
                              in_table TEXT,
                              prediction_col TEXT, observed_col TEXT,
                              out_table TEXT)
RETURNS VOID

in_table       - Name of the input table.
prediction_col - Name of the column of predicted values.
observed_col   - Name of the column of observed values.
out_table      - Name of the output table.

Usage
=====
In multi-class classification, observations and predictions are taken from
some finite set. Elements of this set may be encoded in any form, e.g. as
numbers or strings. Each element is known as a 'class'.
A confusion matrix, which is the output of this function, details for each
'x' and 'y', how many times in the data given in 'in_table', a
prediction of 'x' corresponded to an observed actual value of 'y'.
The function can perform this regardless of the type of data in the
prediction and observed columns, but this type should be the same in both.

The output is a table with two columns:
 * 'class' - The observed class.
 * 'confusion_arr' - an array of integers, each integer corresponding to
                     the number of times a particular predicted value
                     corresponded to the observed class. The array is
                     ordered by the predicted value.

The 'confusion_arr' rows, when ordered by the observed class, form a
square matrix whose diagonal corresponds to 'correct' classifications. The
matrix includes rows (and columns) for each class represented in
'in_table', whether it was seen as an observed or as a predicted class.

Example
=======
Assume we have trained a model and used it to perform multi-class
classification on certain values of the independent parameters.
(In the example, these values are taken to be from a "test_set", implying
that they are distinct from those values on which the model was trained.
However, in real usage the function may be used also on the training set,
although the interpretation of the results will be different.)
In parallel, for each set of values of the independent parameters
we also have the ground truth, e.g. an observed value.

In the example, the classes are the numbers 0 through 5. However, to
demonstrate how the UDF functions when there is incomplete overlap between
predicted and observed values, we will only use 1 through 5 for predictions
and only use 0 through 4 for observed values.

We place the predicted and observed values into the "pred" and "obs"
columns of the "test_set" table, respectively.

For the example, we will create the following dummy test_set table:

user=$ CREATE TABLE test_set AS
user-$   SELECT (x+y)%5+1 AS pred,
user-$          (x*y)%5 AS obs
user-$    FROM generate_series(1,5) x,
user-$         generate_series(1,5) y
user-$ DISTRIBUTED RANDOMLY;

The confusion matrix is generated as follows.

user=# SELECT mf_confusion_matrix('test_set','pred','obs','out_table');
 mf_confusion_matrix 
---------------------
 
(1 row)

michaeldb=# SELECT * FROM out_table ORDER BY class;
 class | confusion_arr 
-------+---------------
     0 | {0,1,2,2,2,2}
     1 | {0,2,0,1,1,0}
     2 | {0,0,0,2,2,0}
     3 | {0,0,2,0,0,2}
     4 | {0,2,1,0,0,1}
     5 | {0,0,0,0,0,0}
(6 rows)

As can be seen, the generated matrix contains rows and columns for all values
between 0 and 5, even though some were never seen as observed classes and
others were never seen as predicted classes.

$ABC$::TEXT
END;
$$ LANGUAGE SQL;

